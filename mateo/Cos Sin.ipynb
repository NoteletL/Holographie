{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb4fb22-80a7-47a5-b78c-3ad099c73875",
   "metadata": {},
   "source": [
    "# Imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4062e76-614a-42cd-b2d6-c3f1948211dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08122e4-8358-403c-96d5-343bfae54e52",
   "metadata": {},
   "source": [
    "# Config :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7592b939-5353-4c09-b792-cb80c2e2b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "img_size = (128, 128)\n",
    "num_noise_steps = 450\n",
    "beta_min = 1e-4\n",
    "beta_max = 0.02\n",
    "variance_schedule = np.linspace(beta_min, beta_max, num_noise_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70007475-b6c8-4c3b-81cc-e737d7db3b3d",
   "metadata": {},
   "source": [
    "# Diffusion Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dbe8259-692f-4eda-bbd6-30dac131c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=10000):\n",
    "        super(SinusoidalPositionEncoding, self).__init__()\n",
    "        self.dim = dim\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, t):\n",
    "        return self.pe[t]\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_channels, filter_size, num_groups, name):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.norm1 = nn.GroupNorm(num_groups, num_channels)\n",
    "        self.swish = nn.SiLU()  # Utilisation de SiLU comme approximation de Swish\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=filter_size, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(num_groups, num_channels)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=filter_size, padding=1)\n",
    "        self.fc = nn.Linear(256, num_channels)  # Ajuste la taille de l'`embedding` à `num_channels`\n",
    "\n",
    "    def forward(self, x, embedding):\n",
    "        # Normalisation et activation\n",
    "        out = self.norm1(x)\n",
    "        out = self.swish(out)\n",
    "        out = self.conv1(out)\n",
    "        if len(embedding.shape) == 1:  # Si embedding est de la forme [256]\n",
    "            embedding = embedding.unsqueeze(0)  # Redimensionner en [1, 256] pour un seul batch\n",
    "        if len(embedding.shape) == 2:  # Si l'`embedding` est (batch_size, 256)\n",
    "            embedding = self.fc(embedding)  # (batch_size, num_channels)\n",
    "        embedding = embedding.unsqueeze(-1).unsqueeze(-1)\n",
    "        embedding = embedding.expand(-1, -1, out.shape[2], out.shape[3])  # Diffusion (batch_size, num_channels, H, W)\n",
    "        out = out + embedding\n",
    "        out = self.norm2(out)\n",
    "        out = self.swish(out)\n",
    "        out = self.conv2(out)\n",
    "        return x + out \n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, num_heads, num_key_channels, num_groups, name):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups, num_key_channels)\n",
    "        self.self_attention = nn.MultiheadAttention(num_key_channels, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_flat = x.view(B, C, H * W).permute(2, 0, 1)  # Reshape en [HW, B, C]\n",
    "        out, _ = self.self_attention(x_flat, x_flat, x_flat)\n",
    "        out = out.permute(1, 2, 0).view(B, C, H, W)  # Retour en [B, C, H, W]\n",
    "        return x + out  # Skip connection\n",
    "\n",
    "\n",
    "\n",
    "class DiffusionUNet(nn.Module):\n",
    "    def __init__(self, num_image_channels=1, initial_num_channels=64, num_groups=32, num_heads=1):\n",
    "        super(DiffusionUNet, self).__init__()\n",
    "        self.initial_num_channels = initial_num_channels\n",
    "        self.conv_in = nn.Conv2d(num_image_channels, initial_num_channels, kernel_size=3, padding=1)\n",
    "        self.res_block1 = ResidualBlock(initial_num_channels, 3, num_groups, \"1\")\n",
    "        self.res_block2 = ResidualBlock(initial_num_channels, 3, num_groups, \"2\")\n",
    "        self.downsample2 = nn.Conv2d(initial_num_channels, 2 * initial_num_channels, kernel_size=3, padding=1, stride=2)\n",
    "        self.res_block3 = ResidualBlock(2 * initial_num_channels, 3, num_groups, \"3\")\n",
    "        self.attn_block3 = AttentionBlock(num_heads, 2 * initial_num_channels, num_groups, \"3\")\n",
    "        self.downsample4 = nn.Conv2d(2 * initial_num_channels, 4 * initial_num_channels, kernel_size=3, padding=1, stride=2)\n",
    "        self.res_block5 = ResidualBlock(4 * initial_num_channels, 3, num_groups, \"5\")\n",
    "        self.res_block7 = ResidualBlock(4 * initial_num_channels, 3, num_groups, \"7\")\n",
    "        self.attn_block7 = AttentionBlock(num_heads, 4 * initial_num_channels, num_groups, \"7\")\n",
    "        self.upsample4 = nn.ConvTranspose2d(4 * initial_num_channels, 2 * initial_num_channels, kernel_size=2, stride=2)\n",
    "        self.res_block9 = ResidualBlock(2 * initial_num_channels, 3, num_groups, \"9\")\n",
    "        self.upsample2 = nn.ConvTranspose2d(2 * initial_num_channels, initial_num_channels, kernel_size=2, stride=2)\n",
    "        self.res_block11 = ResidualBlock(initial_num_channels, 3, num_groups, \"11\")\n",
    "        self.conv_out = nn.Conv2d(initial_num_channels, num_image_channels, kernel_size=3, padding=1)\n",
    "        self.position_encoding = SinusoidalPositionEncoding(4 * initial_num_channels)\n",
    "        self.fc_embed = nn.Sequential(\n",
    "            nn.Linear(4 * initial_num_channels, 4 * initial_num_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(4 * initial_num_channels, 4 * initial_num_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Encode l'étape de bruit\n",
    "        t_emb = self.position_encoding(t)\n",
    "        t_emb = self.fc_embed(t_emb)\n",
    "        x1 = self.conv_in(x)\n",
    "        x2 = self.res_block1(x1, t_emb)\n",
    "        x3 = self.res_block2(x2, t_emb)\n",
    "        x4 = self.downsample2(x3)\n",
    "        x5 = self.res_block3(x4, t_emb)\n",
    "        x6 = self.attn_block3(x5)\n",
    "        x7 = self.downsample4(x6)\n",
    "        x8 = self.res_block7(x7, t_emb)\n",
    "        x9 = self.attn_block7(x8)\n",
    "        x10 = self.upsample4(x9)\n",
    "        x11 = self.res_block9(x10 + x6, t_emb)\n",
    "        x12 = self.upsample2(x11)\n",
    "        x13 = self.res_block11(x12 + x3, t_emb)\n",
    "        output = self.conv_out(x13 + x1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bee1ff-0e8f-4164-b480-e2e1617cc621",
   "metadata": {},
   "source": [
    "# Bruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4237448a-ab0c-4cb1-a3fa-a205bb2cb3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise_to_image(img, noise, noise_step, variance_schedule):# Fonction pour appliquer du bruit à une image\n",
    "    alpha_bar = np.cumprod(1 - variance_schedule)\n",
    "    alpha_bar_t = alpha_bar[noise_step]\n",
    "    noisy_img = torch.sqrt(torch.tensor(alpha_bar_t)) * img + torch.sqrt(1 - torch.tensor(alpha_bar_t)) * noise\n",
    "    return noisy_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e757055-668f-4b51-a176-8387083284a2",
   "metadata": {},
   "source": [
    "# NetCos et NetSin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef7d3f27-4586-47ea-8054-c0f75d797609",
   "metadata": {},
   "outputs": [],
   "source": [
    "netCos = DiffusionUNet()  # instanciation du modèle\n",
    "state_dict = torch.load('Models/128Cos.pth')#Chargement du modèle\n",
    "netCos.load_state_dict(state_dict)# Charger les poids dans le modèle\n",
    "netCos = netCos.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))# Déplacer le modèle vers le bon appareil (GPU ou CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "856452a8-b7f2-494e-a066-54d5c1212ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "netSin = DiffusionUNet()  # instanciation du modèle\n",
    "state_dict = torch.load('Models/128Sin.pth')#Chargement du modèle\n",
    "netSin.load_state_dict(state_dict)# Charger les poids dans le modèle\n",
    "netSin = netSin.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))# Déplacer le modèle vers le bon appareil (GPU ou CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d1309-f19f-4677-be1e-c34bc7184e65",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f9b1770-3d14-44d8-8b50-0945133e8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageclean = 'Datas/NoiseSIN/sin_ImageBruite1.tif.png'\n",
    "imagenoise = 'Datas/NoiseCOS/cos_ImageBruite1.tif.png'\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_images_from_directory(img_path, transform):\n",
    "    if os.path.isfile(img_path):\n",
    "        image = Image.open(img_path).convert('L')  # Load as grayscale\n",
    "        image = transform(image)\n",
    "        return image\n",
    "\n",
    "# Load the images\n",
    "Cos_noise_img = load_images_from_directory(imagenoise, transform)\n",
    "Sin_noise_img = load_images_from_directory(imageclean, transform)\n",
    "imgs = [clean_img, noise_img]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896b488-b0bc-437b-946f-62d585b143e8",
   "metadata": {},
   "source": [
    "# Fonction génération d'image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6da814e9-33c9-42bf-90de-dae0d60c4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(net, variance_schedule, img):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if len(img.shape) == 3:  # img of shape [num_channels, height, width]\n",
    "            img = img.unsqueeze(0)  # Becomes [1, num_channels, height, width]\n",
    "\n",
    "        # Move images to the correct device (GPU or CPU)\n",
    "        images = img.to(device)\n",
    "\n",
    "        # Calculate alpha_bar and posterior variance\n",
    "        alpha_bar = np.cumprod(1 - variance_schedule)\n",
    "        alpha_bar_prev = np.hstack([1, alpha_bar[:-1]])\n",
    "        posterior_variance = variance_schedule * (1 - alpha_bar_prev) / (1 - alpha_bar)\n",
    "\n",
    "        for noise_step in reversed(range(len(variance_schedule))):  # Iterate over 250 steps\n",
    "            z = torch.zeros_like(images)  # Set z to zero, avoiding the addition of noise\n",
    "\n",
    "            predicted_noise = net(images, noise_step)\n",
    "            sqrt_one_minus_beta = torch.sqrt(torch.tensor(1 - variance_schedule[noise_step], device=device))\n",
    "            pred_noise = (torch.tensor(variance_schedule[noise_step], device=device) * predicted_noise / \n",
    "                          torch.sqrt(torch.tensor(1 - alpha_bar[noise_step], device=device)))\n",
    "\n",
    "            # Update images without adding noise\n",
    "            images = (1 / sqrt_one_minus_beta) * (images - pred_noise) + \\\n",
    "                     (torch.sqrt(torch.tensor(posterior_variance[noise_step], device=device)) * z)\n",
    "\n",
    "            # Ensure images are 4D for Conv2D: [batch_size, channels, height, width]\n",
    "            if images.dim() == 5:\n",
    "                images = images.squeeze(1)  # Remove the second dimension (if it's size 1)\n",
    "            if images.dim() != 4:\n",
    "                raise ValueError(f\"Expected 4D input for Conv2D, but got {images.dim()}D tensor.\")\n",
    "    return images.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddba44a-7054-42ef-a2e4-408e4d056f5f",
   "metadata": {},
   "source": [
    "# Génération de patch puis diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7b124d1-0b12-4989-a50c-ce4e1155273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffPatch(net,noise_img):\n",
    "    # Dimensions de l'image\n",
    "    image_size = 512\n",
    "    patch_size = 128\n",
    "    recouvrement = 64\n",
    "    \n",
    "    # Calculer le pas basé sur le recouvrement\n",
    "    pas = int((recouvrement / 100) * patch_size)\n",
    "    \n",
    "    # Créer les matrices pour l'image finale et les poids\n",
    "    imageFinal = np.zeros((image_size, image_size))\n",
    "    poidsImage = np.zeros((image_size, image_size))\n",
    "    \n",
    "    # Calculer les indices des patches\n",
    "    patch_indices = [(i, j) for j in range(0, image_size - pas, pas) for i in range(0, image_size - pas, pas)]\n",
    "    \n",
    "    # Préparer une liste pour les images générées\n",
    "    images_générées = []\n",
    "    \n",
    "    # Générer toutes les images pour chaque patch\n",
    "    for (i, j) in patch_indices:\n",
    "        patch_input = noise_img[:1, i:i + patch_size, j:j + patch_size]  # Extraire le patch\n",
    "        image_générée = generate_image(net, variance_schedule, patch_input)  # Générer l'image pour le patch\n",
    "        images_générées.append((i, j, image_générée))  # Stocker l'image générée avec ses indices\n",
    "    \n",
    "    # Mettre à jour l'image finale et les poids\n",
    "    for (i, j, image_générée) in images_générées:\n",
    "        imageFinal[i:i + patch_size, j:j + patch_size] += image_générée.squeeze()  # Mettre à jour l'image finale\n",
    "        poidsImage[i:i + patch_size, j:j + patch_size] += 1  # Incrémenter le poids\n",
    "    \n",
    "    # Après cela, tu peux calculer la moyenne si nécessaire\n",
    "    poidsImage[poidsImage == 0] = 1  # Remplacer les zéros par 1 pour éviter la division par zéro\n",
    "    imageFinal /= poidsImage  # Normalisation\n",
    "    return imageFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a568451-e2ed-4a4f-945a-e2fea4c6393e",
   "metadata": {},
   "source": [
    "# Diffusion sur Cos et Sin pour une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b235b0e5-8c7a-406b-b8bf-7a9f075a9edc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (108) must match the size of tensor b (107) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m diff_Sin \u001b[38;5;241m=\u001b[39m \u001b[43mdiffPatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetSin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSin_noise_img\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mdiffPatch\u001b[0;34m(net, noise_img)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, j) \u001b[38;5;129;01min\u001b[39;00m patch_indices:\n\u001b[1;32m     22\u001b[0m     patch_input \u001b[38;5;241m=\u001b[39m noise_img[:\u001b[38;5;241m1\u001b[39m, i:i \u001b[38;5;241m+\u001b[39m patch_size, j:j \u001b[38;5;241m+\u001b[39m patch_size]  \u001b[38;5;66;03m# Extraire le patch\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     image_générée \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariance_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Générer l'image pour le patch\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     images_générées\u001b[38;5;241m.\u001b[39mappend((i, j, image_générée))  \u001b[38;5;66;03m# Stocker l'image générée avec ses indices\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Mettre à jour l'image finale et les poids\u001b[39;00m\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mgenerate_image\u001b[0;34m(net, variance_schedule, img)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noise_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(variance_schedule))):  \u001b[38;5;66;03m# Iterate over 250 steps\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(images)  \u001b[38;5;66;03m# Set z to zero, avoiding the addition of noise\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     sqrt_one_minus_beta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m variance_schedule[noise_step], device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     21\u001b[0m     pred_noise \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mtensor(variance_schedule[noise_step], device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m predicted_noise \u001b[38;5;241m/\u001b[39m \n\u001b[1;32m     22\u001b[0m                   torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_bar[noise_step], device\u001b[38;5;241m=\u001b[39mdevice)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mDiffusionUNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     96\u001b[0m x11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_block9(x10 \u001b[38;5;241m+\u001b[39m x6, t_emb)\n\u001b[1;32m     97\u001b[0m x12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample2(x11)\n\u001b[0;32m---> 98\u001b[0m x13 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_block11(\u001b[43mx12\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m, t_emb)\n\u001b[1;32m     99\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_out(x13 \u001b[38;5;241m+\u001b[39m x1)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (108) must match the size of tensor b (107) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "diff_Sin = diffPatch(netSin,Sin_noise_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8a399-785c-408e-8897-35570993690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_Cos = diffPatch(netCos,Cos_noise_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9157d-e3c9-4ff1-b23b-3446f66de27d",
   "metadata": {},
   "source": [
    "# Reconstitution de la Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca90af-c226-4773-bfd7-da854fcfacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_image = np.angle(diff_Cos + (diff_Sin * 1j))\n",
    "plt.figure(figsize=(5, 5))  # Ajuster la taille ici (en pouces)\n",
    "plt.imshow(phase_image, cmap='gray')\n",
    "plt.title('Image de Phase Reconstituée')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81e0cc-81d6-4547-9de9-7fc8fc82820a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
